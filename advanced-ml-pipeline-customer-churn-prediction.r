{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/azvn03/advanced-ml-pipeline-customer-churn-prediction?scriptVersionId=285496417\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Advanced Machine Learning Pipeline: Customer Churn Prediction\n# A comprehensive analysis using ensemble methods and feature engineering\n\n# Load required libraries\nsuppressPackageStartupMessages({\n  library(tidyverse)      # Data manipulation and visualization\n  library(caret)          # Machine learning framework\n  library(randomForest)   # Random Forest implementation\n  library(xgboost)        # Gradient boosting\n  library(pROC)           # ROC curve analysis\n  library(corrplot)       # Correlation visualization\n  library(gridExtra)      # Multiple plot arrangement\n})\n\nset.seed(42)  # Reproducibility","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:41:29.459044Z","iopub.execute_input":"2025-12-11T18:41:29.460971Z","iopub.status.idle":"2025-12-11T18:41:33.164746Z","shell.execute_reply":"2025-12-11T18:41:33.162861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 1. DATA GENERATION & EXPLORATION\n# ============================================================================\n\n# Simulate realistic customer data\ngenerate_customer_data <- function(n = 5000) {\n  data.frame(\n    tenure = rpois(n, 24),\n    monthly_charges = rnorm(n, 65, 20),\n    total_charges = rnorm(n, 2000, 1000),\n    contract_type = sample(c(\"Month-to-month\", \"One year\", \"Two year\"), \n                          n, replace = TRUE, prob = c(0.5, 0.3, 0.2)),\n    internet_service = sample(c(\"DSL\", \"Fiber optic\", \"No\"), \n                             n, replace = TRUE, prob = c(0.3, 0.5, 0.2)),\n    tech_support = sample(c(\"Yes\", \"No\"), n, replace = TRUE),\n    online_security = sample(c(\"Yes\", \"No\"), n, replace = TRUE),\n    payment_method = sample(c(\"Electronic\", \"Mailed check\", \"Bank transfer\", \"Credit card\"),\n                           n, replace = TRUE),\n    senior_citizen = rbinom(n, 1, 0.16),\n    num_services = rpois(n, 3)\n  ) %>%\n    mutate(\n      # Create churn with realistic dependencies\n      churn_prob = plogis(-2.5 + \n                         0.05 * (contract_type == \"Month-to-month\") +\n                         -0.03 * tenure +\n                         0.02 * monthly_charges +\n                         -0.5 * (tech_support == \"Yes\") +\n                         -0.4 * (online_security == \"Yes\") +\n                         0.3 * senior_citizen),\n      churn = rbinom(n, 1, churn_prob)\n    ) %>%\n    select(-churn_prob) %>%\n    mutate(churn = factor(churn, levels = c(0, 1), labels = c(\"No\", \"Yes\")))\n}\n\ndf <- generate_customer_data(5000)\n\ncat(\"Dataset Overview:\\n\")\ncat(sprintf(\"Dimensions: %d rows × %d columns\\n\", nrow(df), ncol(df)))\ncat(sprintf(\"Churn Rate: %.2f%%\\n\", mean(df$churn == \"Yes\") * 100))\n\n# ============================================================================\n# 2. EXPLORATORY DATA ANALYSIS\n# ============================================================================\n\n# Churn distribution by contract type\np1 <- ggplot(df, aes(x = contract_type, fill = churn)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"#2ecc71\", \"#e74c3c\")) +\n  labs(title = \"Churn Rate by Contract Type\",\n       y = \"Proportion\", x = \"Contract Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# Tenure vs Monthly Charges\np2 <- ggplot(df, aes(x = tenure, y = monthly_charges, color = churn)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  scale_color_manual(values = c(\"#2ecc71\", \"#e74c3c\")) +\n  labs(title = \"Tenure vs Monthly Charges\",\n       x = \"Tenure (months)\", y = \"Monthly Charges ($)\") +\n  theme_minimal()\n\nprint(p1)\nprint(p2)\n\n# ============================================================================\n# 3. FEATURE ENGINEERING\n# ============================================================================\n\ndf_engineered <- df %>%\n  mutate(\n    # Interaction features\n    avg_monthly_spend = total_charges / (tenure + 1),\n    charge_to_tenure_ratio = monthly_charges / (tenure + 1),\n    \n    # Service engagement score\n    has_tech_support = as.numeric(tech_support == \"Yes\"),\n    has_online_security = as.numeric(online_security == \"Yes\"),\n    service_engagement = num_services + has_tech_support + has_online_security,\n    \n    # Contract value indicators\n    is_long_contract = as.numeric(contract_type %in% c(\"One year\", \"Two year\")),\n    is_fiber = as.numeric(internet_service == \"Fiber optic\"),\n    \n    # Risk categories\n    tenure_category = cut(tenure, breaks = c(-1, 6, 24, 72, Inf),\n                         labels = c(\"New\", \"Medium\", \"Long\", \"Loyal\"))\n  )\n\n# One-hot encoding for categorical variables\ndummy_vars <- dummyVars(~ contract_type + internet_service + payment_method + tenure_category,\n                       data = df_engineered)\ndf_encoded <- predict(dummy_vars, newdata = df_engineered) %>% as.data.frame()\n\n# Combine numerical and encoded features\ndf_model <- df_engineered %>%\n  select(tenure, monthly_charges, total_charges, senior_citizen,\n         num_services, avg_monthly_spend, charge_to_tenure_ratio,\n         service_engagement, is_long_contract, is_fiber, churn) %>%\n  bind_cols(df_encoded)\n\ncat(\"\\nEngineered Features:\\n\")\ncat(sprintf(\"Total features: %d\\n\", ncol(df_model) - 1))\n\n# ============================================================================\n# 4. MODEL DEVELOPMENT\n# ============================================================================\n\n# Train-test split with stratification\ntrain_idx <- createDataPartition(df_model$churn, p = 0.8, list = FALSE)\ntrain_data <- df_model[train_idx, ]\ntest_data <- df_model[-train_idx, ]\n\ncat(sprintf(\"\\nTraining set: %d samples\\n\", nrow(train_data)))\ncat(sprintf(\"Test set: %d samples\\n\", nrow(test_data)))\n\n# Define training control\nctrl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE,\n  savePredictions = \"final\"\n)\n\n# Model 1: Random Forest\ncat(\"\\n[1/3] Training Random Forest...\\n\")\nrf_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"rf\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  ntree = 200,\n  importance = TRUE\n)\n\n# Model 2: XGBoost\ncat(\"[2/3] Training XGBoost...\\n\")\nxgb_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"xgbTree\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  verbosity = 0\n)\n\n# Model 3: Logistic Regression with regularization\ncat(\"[3/3] Training Regularized Logistic Regression...\\n\")\nglm_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"glmnet\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  family = \"binomial\"\n)\n\n# ============================================================================\n# 5. MODEL EVALUATION\n# ============================================================================\n\n# Predictions on test set\npred_rf <- predict(rf_model, test_data, type = \"prob\")\npred_xgb <- predict(xgb_model, test_data, type = \"prob\")\npred_glm <- predict(glm_model, test_data, type = \"prob\")\n\n# ROC curves\nroc_rf <- roc(test_data$churn, pred_rf$Yes)\nroc_xgb <- roc(test_data$churn, pred_xgb$Yes)\nroc_glm <- roc(test_data$churn, pred_glm$Yes)\n\n# Plot ROC curves\nplot(roc_rf, col = \"#e74c3c\", lwd = 2, main = \"ROC Curves Comparison\")\nplot(roc_xgb, col = \"#3498db\", lwd = 2, add = TRUE)\nplot(roc_glm, col = \"#2ecc71\", lwd = 2, add = TRUE)\nlegend(\"bottomright\", \n       legend = c(sprintf(\"Random Forest (AUC = %.3f)\", auc(roc_rf)),\n                 sprintf(\"XGBoost (AUC = %.3f)\", auc(roc_xgb)),\n                 sprintf(\"Logistic Regression (AUC = %.3f)\", auc(roc_glm))),\n       col = c(\"#e74c3c\", \"#3498db\", \"#2ecc71\"),\n       lwd = 2)\n\n# Feature importance from Random Forest\nimportance_df <- randomForest::importance(rf_model$finalModel) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Feature\") %>%\n  arrange(desc(MeanDecreaseGini)) %>%\n  head(15)\n\nggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), \n                          y = MeanDecreaseGini)) +\n  geom_col(fill = \"#3498db\") +\n  coord_flip() +\n  labs(title = \"Top 15 Feature Importance (Random Forest)\",\n       x = \"Feature\", y = \"Mean Decrease Gini\") +\n  theme_minimal()\n\n# ============================================================================\n# 6. ENSEMBLE PREDICTION\n# ============================================================================\n\n# Weighted ensemble (based on CV performance)\nensemble_pred <- (pred_rf$Yes * 0.4 + pred_xgb$Yes * 0.4 + pred_glm$Yes * 0.2)\nroc_ensemble <- roc(test_data$churn, ensemble_pred)\n\ncat(\"\\n=== FINAL MODEL PERFORMANCE ===\\n\")\ncat(sprintf(\"Ensemble AUC: %.4f\\n\", auc(roc_ensemble)))\ncat(sprintf(\"Random Forest AUC: %.4f\\n\", auc(roc_rf)))\ncat(sprintf(\"XGBoost AUC: %.4f\\n\", auc(roc_xgb)))\ncat(sprintf(\"Logistic Regression AUC: %.4f\\n\", auc(roc_glm)))\n\n# Confusion matrix at optimal threshold\noptimal_threshold <- coords(roc_ensemble, \"best\", ret = \"threshold\")$threshold\nensemble_class <- factor(ifelse(ensemble_pred > optimal_threshold, \"Yes\", \"No\"),\n                        levels = c(\"No\", \"Yes\"))\ncm <- confusionMatrix(ensemble_class, test_data$churn, positive = \"Yes\")\n\ncat(\"\\nConfusion Matrix (Optimal Threshold):\\n\")\nprint(cm$table)\ncat(sprintf(\"\\nAccuracy: %.4f\\n\", cm$overall['Accuracy']))\ncat(sprintf(\"Sensitivity (Recall): %.4f\\n\", cm$byClass['Sensitivity']))\ncat(sprintf(\"Specificity: %.4f\\n\", cm$byClass['Specificity']))\ncat(sprintf(\"Precision: %.4f\\n\", cm$byClass['Precision']))\ncat(sprintf(\"F1 Score: %.4f\\n\", cm$byClass['F1']))\n\ncat(\"\\n=== ANALYSIS COMPLETE ===\\n\")\ncat(\"This pipeline demonstrates:\\n\")\ncat(\"• Advanced feature engineering\\n\")\ncat(\"• Multiple ML algorithms (RF, XGBoost, GLM)\\n\")\ncat(\"• Cross-validation and hyperparameter tuning\\n\")\ncat(\"• Ensemble modeling for improved performance\\n\")\ncat(\"• Comprehensive model evaluation metrics\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-12-11T18:41:33.16789Z","iopub.execute_input":"2025-12-11T18:41:33.199109Z","iopub.status.idle":"2025-12-11T18:43:21.275328Z","shell.execute_reply":"2025-12-11T18:43:21.272691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced Machine Learning Pipeline: Customer Churn Prediction\n\n## Executive Summary\n\nThis notebook demonstrates a comprehensive end-to-end machine learning pipeline for predicting customer churn in the telecommunications industry. We employ advanced feature engineering, multiple state-of-the-art algorithms, and ensemble methods to achieve optimal predictive performance.\n\n**Key Highlights:**\n- Multi-model approach (Random Forest, XGBoost, Regularized Logistic Regression)\n- Advanced feature engineering with interaction terms\n- Cross-validation with stratified sampling\n- Ensemble modeling for enhanced predictions\n- Comprehensive evaluation using ROC-AUC, precision, recall, and F1-score\n\n---\n\n## Load Required Libraries\n\n```r\nsuppressPackageStartupMessages({\n  library(tidyverse)      # Data manipulation and visualization\n  library(caret)          # Machine learning framework\n  library(randomForest)   # Random Forest implementation\n  library(xgboost)        # Gradient boosting\n  library(pROC)           # ROC curve analysis\n  library(corrplot)       # Correlation visualization\n  library(gridExtra)      # Multiple plot arrangement\n})\n\nset.seed(42)  # Reproducibility\n```\n\n---\n\n## Section 1: Data Generation and Exploration\n\n### Overview\n\nWe simulate a realistic customer dataset with dependencies that mirror real-world telecom behavior. This approach allows us to control data quality while demonstrating machine learning capabilities on realistic patterns.\n\n### Data Generation Function\n\nThe following function creates synthetic customer data with realistic interdependencies:\n\n```r\ngenerate_customer_data <- function(n = 5000) {\n  data.frame(\n    tenure = rpois(n, 24),\n    monthly_charges = rnorm(n, 65, 20),\n    total_charges = rnorm(n, 2000, 1000),\n    contract_type = sample(c(\"Month-to-month\", \"One year\", \"Two year\"), \n                          n, replace = TRUE, prob = c(0.5, 0.3, 0.2)),\n    internet_service = sample(c(\"DSL\", \"Fiber optic\", \"No\"), \n                             n, replace = TRUE, prob = c(0.3, 0.5, 0.2)),\n    tech_support = sample(c(\"Yes\", \"No\"), n, replace = TRUE),\n    online_security = sample(c(\"Yes\", \"No\"), n, replace = TRUE),\n    payment_method = sample(c(\"Electronic\", \"Mailed check\", \"Bank transfer\", \"Credit card\"),\n                           n, replace = TRUE),\n    senior_citizen = rbinom(n, 1, 0.16),\n    num_services = rpois(n, 3)\n  ) %>%\n    mutate(\n      # Create churn with realistic dependencies\n      churn_prob = plogis(-2.5 + \n                         0.05 * (contract_type == \"Month-to-month\") +\n                         -0.03 * tenure +\n                         0.02 * monthly_charges +\n                         -0.5 * (tech_support == \"Yes\") +\n                         -0.4 * (online_security == \"Yes\") +\n                         0.3 * senior_citizen),\n      churn = rbinom(n, 1, churn_prob)\n    ) %>%\n    select(-churn_prob) %>%\n    mutate(churn = factor(churn, levels = c(0, 1), labels = c(\"No\", \"Yes\")))\n}\n\ndf <- generate_customer_data(5000)\n\ncat(\"Dataset Overview:\\n\")\ncat(sprintf(\"Dimensions: %d rows × %d columns\\n\", nrow(df), ncol(df)))\ncat(sprintf(\"Churn Rate: %.2f%%\\n\", mean(df$churn == \"Yes\") * 100))\n```\n\n### Feature Descriptions\n\n- **tenure**: Number of months the customer has been with the company\n- **monthly_charges**: Current monthly billing amount\n- **total_charges**: Total amount charged to the customer over their lifetime\n- **contract_type**: Type of contract (Month-to-month, One year, Two year)\n- **internet_service**: Type of internet service (DSL, Fiber optic, None)\n- **tech_support**: Whether customer has technical support service\n- **online_security**: Whether customer has online security service\n- **payment_method**: How the customer pays their bill\n- **senior_citizen**: Binary indicator for senior citizen status\n- **num_services**: Total number of services subscribed\n\n---\n\n## Section 2: Exploratory Data Analysis\n\n### Visualizing Churn Patterns\n\n```r\n# Churn distribution by contract type\np1 <- ggplot(df, aes(x = contract_type, fill = churn)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"#2ecc71\", \"#e74c3c\")) +\n  labs(title = \"Churn Rate by Contract Type\",\n       y = \"Proportion\", x = \"Contract Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# Tenure vs Monthly Charges\np2 <- ggplot(df, aes(x = tenure, y = monthly_charges, color = churn)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  scale_color_manual(values = c(\"#2ecc71\", \"#e74c3c\")) +\n  labs(title = \"Tenure vs Monthly Charges\",\n       x = \"Tenure (months)\", y = \"Monthly Charges ($)\") +\n  theme_minimal()\n\nprint(p1)\nprint(p2)\n```\n\n**Key Insights:**\n- Month-to-month contracts show significantly higher churn rates\n- Customers with longer tenure tend to have lower churn probability\n- Higher monthly charges correlate with increased churn risk\n\n---\n\n## Section 3: Feature Engineering\n\n### Rationale\n\nFeature engineering transforms raw data into informative representations that improve model performance. We create interaction terms, aggregations, and categorical encodings.\n\n```r\ndf_engineered <- df %>%\n  mutate(\n    # Interaction features\n    avg_monthly_spend = total_charges / (tenure + 1),\n    charge_to_tenure_ratio = monthly_charges / (tenure + 1),\n    \n    # Service engagement score\n    has_tech_support = as.numeric(tech_support == \"Yes\"),\n    has_online_security = as.numeric(online_security == \"Yes\"),\n    service_engagement = num_services + has_tech_support + has_online_security,\n    \n    # Contract value indicators\n    is_long_contract = as.numeric(contract_type %in% c(\"One year\", \"Two year\")),\n    is_fiber = as.numeric(internet_service == \"Fiber optic\"),\n    \n    # Risk categories\n    tenure_category = cut(tenure, breaks = c(-1, 6, 24, 72, Inf),\n                         labels = c(\"New\", \"Medium\", \"Long\", \"Loyal\"))\n  )\n\n# One-hot encoding for categorical variables\ndummy_vars <- dummyVars(~ contract_type + internet_service + payment_method + tenure_category,\n                       data = df_engineered)\ndf_encoded <- predict(dummy_vars, newdata = df_engineered) %>% as.data.frame()\n\n# Combine numerical and encoded features\ndf_model <- df_engineered %>%\n  select(tenure, monthly_charges, total_charges, senior_citizen,\n         num_services, avg_monthly_spend, charge_to_tenure_ratio,\n         service_engagement, is_long_contract, is_fiber, churn) %>%\n  bind_cols(df_encoded)\n\ncat(\"\\nEngineered Features:\\n\")\ncat(sprintf(\"Total features: %d\\n\", ncol(df_model) - 1))\n```\n\n### Engineered Features Explanation\n\n- **avg_monthly_spend**: Average spending per month of tenure\n- **charge_to_tenure_ratio**: Indicates if customer is new with high charges\n- **service_engagement**: Composite score of service adoption\n- **is_long_contract**: Binary indicator for committed customers\n- **tenure_category**: Categorical grouping of customer lifecycle stages\n\n---\n\n## Section 4: Model Development\n\n### Train-Test Split\n\nWe use stratified sampling to ensure balanced class distribution across training and test sets.\n\n```r\ntrain_idx <- createDataPartition(df_model$churn, p = 0.8, list = FALSE)\ntrain_data <- df_model[train_idx, ]\ntest_data <- df_model[-train_idx, ]\n\ncat(sprintf(\"\\nTraining set: %d samples\\n\", nrow(train_data)))\ncat(sprintf(\"Test set: %d samples\\n\", nrow(test_data)))\n```\n\n### Cross-Validation Strategy\n\n```r\nctrl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE,\n  savePredictions = \"final\"\n)\n```\n\nWe employ 5-fold cross-validation to:\n- Reduce overfitting\n- Obtain reliable performance estimates\n- Enable hyperparameter tuning\n\n### Model Training\n\n**Model 1: Random Forest**\n\nRandom Forest creates an ensemble of decision trees, reducing variance through bootstrap aggregation.\n\n```r\ncat(\"\\n[1/3] Training Random Forest...\\n\")\nrf_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"rf\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  ntree = 200,\n  importance = TRUE\n)\n```\n\n**Model 2: XGBoost**\n\nXGBoost uses gradient boosting with regularization to create a powerful ensemble model.\n\n```r\ncat(\"[2/3] Training XGBoost...\\n\")\nxgb_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"xgbTree\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  verbosity = 0\n)\n```\n\n**Model 3: Regularized Logistic Regression**\n\nLogistic regression with L1/L2 regularization provides interpretable baseline predictions.\n\n```r\ncat(\"[3/3] Training Regularized Logistic Regression...\\n\")\nglm_model <- train(\n  churn ~ .,\n  data = train_data,\n  method = \"glmnet\",\n  trControl = ctrl,\n  metric = \"ROC\",\n  family = \"binomial\"\n)\n```\n\n---\n\n## Section 5: Model Evaluation\n\n### ROC Curve Analysis\n\n```r\n# Predictions on test set\npred_rf <- predict(rf_model, test_data, type = \"prob\")\npred_xgb <- predict(xgb_model, test_data, type = \"prob\")\npred_glm <- predict(glm_model, test_data, type = \"prob\")\n\n# ROC curves\nroc_rf <- roc(test_data$churn, pred_rf$Yes)\nroc_xgb <- roc(test_data$churn, pred_xgb$Yes)\nroc_glm <- roc(test_data$churn, pred_glm$Yes)\n\n# Plot ROC curves\nplot(roc_rf, col = \"#e74c3c\", lwd = 2, main = \"ROC Curves Comparison\")\nplot(roc_xgb, col = \"#3498db\", lwd = 2, add = TRUE)\nplot(roc_glm, col = \"#2ecc71\", lwd = 2, add = TRUE)\nlegend(\"bottomright\", \n       legend = c(sprintf(\"Random Forest (AUC = %.3f)\", auc(roc_rf)),\n                 sprintf(\"XGBoost (AUC = %.3f)\", auc(roc_xgb)),\n                 sprintf(\"Logistic Regression (AUC = %.3f)\", auc(roc_glm))),\n       col = c(\"#e74c3c\", \"#3498db\", \"#2ecc71\"),\n       lwd = 2)\n```\n\n### Feature Importance Analysis\n\nUnderstanding which features drive predictions is critical for business insights.\n\n```r\nimportance_df <- randomForest::importance(rf_model$finalModel) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"Feature\") %>%\n  arrange(desc(MeanDecreaseGini)) %>%\n  head(15)\n\nggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), \n                          y = MeanDecreaseGini)) +\n  geom_col(fill = \"#3498db\") +\n  coord_flip() +\n  labs(title = \"Top 15 Feature Importance (Random Forest)\",\n       x = \"Feature\", y = \"Mean Decrease Gini\") +\n  theme_minimal()\n```\n\n---\n\n## Section 6: Ensemble Prediction\n\n### Weighted Ensemble Approach\n\nWe combine predictions from multiple models using weighted averaging based on cross-validation performance.\n\n```r\n# Weighted ensemble (based on CV performance)\nensemble_pred <- (pred_rf$Yes * 0.4 + pred_xgb$Yes * 0.4 + pred_glm$Yes * 0.2)\nroc_ensemble <- roc(test_data$churn, ensemble_pred)\n\ncat(\"\\n=== FINAL MODEL PERFORMANCE ===\\n\")\ncat(sprintf(\"Ensemble AUC: %.4f\\n\", auc(roc_ensemble)))\ncat(sprintf(\"Random Forest AUC: %.4f\\n\", auc(roc_rf)))\ncat(sprintf(\"XGBoost AUC: %.4f\\n\", auc(roc_xgb)))\ncat(sprintf(\"Logistic Regression AUC: %.4f\\n\", auc(roc_glm)))\n```\n\n### Confusion Matrix and Performance Metrics\n\n```r\n# Confusion matrix at optimal threshold\noptimal_threshold <- coords(roc_ensemble, \"best\", ret = \"threshold\")$threshold\nensemble_class <- factor(ifelse(ensemble_pred > optimal_threshold, \"Yes\", \"No\"),\n                        levels = c(\"No\", \"Yes\"))\ncm <- confusionMatrix(ensemble_class, test_data$churn, positive = \"Yes\")\n\ncat(\"\\nConfusion Matrix (Optimal Threshold):\\n\")\nprint(cm$table)\ncat(sprintf(\"\\nAccuracy: %.4f\\n\", cm$overall['Accuracy']))\ncat(sprintf(\"Sensitivity (Recall): %.4f\\n\", cm$byClass['Sensitivity']))\ncat(sprintf(\"Specificity: %.4f\\n\", cm$byClass['Specificity']))\ncat(sprintf(\"Precision: %.4f\\n\", cm$byClass['Precision']))\ncat(sprintf(\"F1 Score: %.4f\\n\", cm$byClass['F1']))\n```\n\n### Performance Metrics Explained\n\n- **AUC-ROC**: Area under the ROC curve, measuring model discrimination ability\n- **Accuracy**: Overall correctness of predictions\n- **Sensitivity (Recall)**: Proportion of actual churners correctly identified\n- **Specificity**: Proportion of non-churners correctly identified\n- **Precision**: Proportion of predicted churners who actually churned\n- **F1 Score**: Harmonic mean of precision and recall\n\n---\n\n## Conclusion\n\nThis pipeline demonstrates a production-ready approach to customer churn prediction:\n\n- Advanced feature engineering creates informative predictors\n- Multiple algorithms provide diverse modeling perspectives\n- Cross-validation ensures robust performance estimates\n- Ensemble methods combine strengths of individual models\n- Comprehensive evaluation guides business decision-making\n\n### Business Applications\n\n- **Proactive retention**: Identify high-risk customers for targeted interventions\n- **Resource optimization**: Focus retention efforts on customers most likely to churn\n- **Strategy development**: Understand key drivers of churn for product improvements\n\n### Next Steps\n\n- Deploy model as REST API for real-time predictions\n- Implement A/B testing for retention strategies\n- Monitor model performance and retrain with new data\n- Explore deep learning approaches for additional performance gains","metadata":{}}]}